{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "bucket = \"nikki-retail-data\"\n",
        "\n",
        "!gsutil cp gs://{bucket}/Walmart/features.csv features.csv\n",
        "!gsutil cp gs://{bucket}/Walmart/sales.csv sales.csv\n",
        "!gsutil cp gs://{bucket}/Walmart/stores.csv stores.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgipMmFUPmNe",
        "outputId": "86696945-aec2-4fa2-eb59-453f213eb107"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://nikki-retail-data/Walmart/features.csv...\n",
            "/ [1 files][  131.0 B/  131.0 B]                                                \n",
            "Operation completed over 1 objects/131.0 B.                                      \n",
            "Copying gs://nikki-retail-data/Walmart/sales.csv...\n",
            "/ [1 files][  100.0 B/  100.0 B]                                                \n",
            "Operation completed over 1 objects/100.0 B.                                      \n",
            "Copying gs://nikki-retail-data/Walmart/stores.csv...\n",
            "/ [1 files][   38.0 B/   38.0 B]                                                \n",
            "Operation completed over 1 objects/38.0 B.                                       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR52ZJovebo2",
        "outputId": "244d6d9b-1590-4ca8-fae4-3d9e2857443c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.11/dist-packages (3.34.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.11.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.38.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.7.2)\n",
            "Requirement already satisfied: packaging>=24.2.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (24.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.26.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (4.9.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-resumable-media<3.0.0,>=2.0.0->google-cloud-bigquery) (1.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2025.6.15)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install pyspark google-cloud-bigquery pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "lI3iiyk2e7CL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark google-cloud-bigquery\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CeUHSWVOLA0",
        "outputId": "a149cc46-8e57-4f0d-d59c-316414c1a647"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.11/dist-packages (3.34.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.11.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.38.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.7.2)\n",
            "Requirement already satisfied: packaging>=24.2.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (24.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.32.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.26.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (4.9.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-resumable-media<3.0.0,>=2.0.0->google-cloud-bigquery) (1.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2025.6.15)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ],
      "metadata": {
        "id": "KB-sRHf_ORxz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RetailSalesForecast\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "IxO4SKdgOjrE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = spark.read.csv(\"features.csv\", header=True, inferSchema=True)\n",
        "sales = spark.read.csv(\"sales.csv\", header=True, inferSchema=True)\n",
        "stores = spark.read.csv(\"stores.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "ySzJd0V5PEAH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.show()\n",
        "sales.show()\n",
        "stores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KqVL576QgNl",
        "outputId": "1f262127-15d5-478c-bf9a-a1bf1dd9ab37"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----------+----------+--------+------------+\n",
            "|Store|      Date|Temperature|Fuel_Price|     CPI|Unemployment|\n",
            "+-----+----------+-----------+----------+--------+------------+\n",
            "|    1|2010-02-05|      42.31|     2.572|211.0964|       8.106|\n",
            "|    1|2010-02-12|      38.51|     2.548|211.2422|       8.106|\n",
            "+-----+----------+-----------+----------+--------+------------+\n",
            "\n",
            "+-----+----+----------+------------+---------+\n",
            "|Store|Dept|      Date|Weekly_Sales|IsHoliday|\n",
            "+-----+----+----------+------------+---------+\n",
            "|    1|   1|2010-02-05|   1643691.0|    false|\n",
            "|    1|   1|2010-02-12|   1641957.0|     true|\n",
            "+-----+----+----------+------------+---------+\n",
            "\n",
            "+-----+----+------+\n",
            "|Store|Type|  Size|\n",
            "+-----+----+------+\n",
            "|    1|   A|151315|\n",
            "|    2|   A|202307|\n",
            "+-----+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, col\n",
        "\n",
        "# Convert date strings to proper Date type\n",
        "features = features.withColumn(\"Date\", to_date(\"Date\", \"yyyy-MM-dd\"))\n",
        "sales = sales.withColumn(\"Date\", to_date(\"Date\", \"yyyy-MM-dd\"))\n",
        "\n",
        "# Join all three datasets\n",
        "df = sales.join(features, [\"Store\", \"Date\"]).join(stores, [\"Store\"])\n",
        "\n",
        "# View the joined schema and a sample of data\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2UF3OwnRG9K",
        "outputId": "4ab4d695-034a-4503-95a3-7c8988d39bbd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Store: integer (nullable = true)\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Dept: integer (nullable = true)\n",
            " |-- Weekly_Sales: double (nullable = true)\n",
            " |-- IsHoliday: boolean (nullable = true)\n",
            " |-- Temperature: double (nullable = true)\n",
            " |-- Fuel_Price: double (nullable = true)\n",
            " |-- CPI: double (nullable = true)\n",
            " |-- Unemployment: double (nullable = true)\n",
            " |-- Type: string (nullable = true)\n",
            " |-- Size: integer (nullable = true)\n",
            "\n",
            "+-----+----------+----+------------+---------+-----------+----------+--------+------------+----+------+\n",
            "|Store|      Date|Dept|Weekly_Sales|IsHoliday|Temperature|Fuel_Price|     CPI|Unemployment|Type|  Size|\n",
            "+-----+----------+----+------------+---------+-----------+----------+--------+------------+----+------+\n",
            "|    1|2010-02-05|   1|   1643691.0|    false|      42.31|     2.572|211.0964|       8.106|   A|151315|\n",
            "|    1|2010-02-12|   1|   1641957.0|     true|      38.51|     2.548|211.2422|       8.106|   A|151315|\n",
            "+-----+----------+----+------------+---------+-----------+----------+--------+------------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ],
      "metadata": {
        "id": "M0lNb3MFO2P5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket = \"nikki-retail-data\"\n",
        "\n",
        "!gsutil cp gs://{bucket}/Walmart/features.csv features.csv\n",
        "!gsutil cp gs://{bucket}/Walmart/sales.csv sales.csv\n",
        "!gsutil cp gs://{bucket}/Walmart/stores.csv stores.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4GJBLbfPArt",
        "outputId": "ed7846b8-75a9-4554-afd9-01728db8346e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://nikki-retail-data/Walmart/features.csv...\n",
            "/ [1 files][  131.0 B/  131.0 B]                                                \n",
            "Operation completed over 1 objects/131.0 B.                                      \n",
            "Copying gs://nikki-retail-data/Walmart/sales.csv...\n",
            "/ [1 files][  100.0 B/  100.0 B]                                                \n",
            "Operation completed over 1 objects/100.0 B.                                      \n",
            "Copying gs://nikki-retail-data/Walmart/stores.csv...\n",
            "/ [1 files][   38.0 B/   38.0 B]                                                \n",
            "Operation completed over 1 objects/38.0 B.                                       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, col\n",
        "\n",
        "features = spark.read.csv(\"features.csv\", header=True, inferSchema=True)\n",
        "sales = spark.read.csv(\"sales.csv\", header=True, inferSchema=True)\n",
        "stores = spark.read.csv(\"stores.csv\", header=True, inferSchema=True)\n",
        "\n",
        "features = features.withColumn(\"Date\", to_date(\"Date\", \"yyyy-MM-dd\"))\n",
        "sales = sales.withColumn(\"Date\", to_date(\"Date\", \"yyyy-MM-dd\"))\n",
        "\n",
        "df = sales.join(features, [\"Store\", \"Date\"]).join(stores, [\"Store\"])\n"
      ],
      "metadata": {
        "id": "KS_6EvNQPJZU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "input_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
        "\n",
        "final_data = assembler.transform(df).select(\"features\", col(\"Weekly_Sales\").alias(\"label\"))\n"
      ],
      "metadata": {
        "id": "L9q-avyJPQoC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Restart your Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RetailSalesForecasting\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "QUPGAk-sNvp5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = spark.read.csv(\"features.csv\", header=True, inferSchema=True)\n",
        "sales = spark.read.csv(\"sales.csv\", header=True, inferSchema=True)\n",
        "stores = spark.read.csv(\"stores.csv\", header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "F7PvydGePmB2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = features.withColumn(\"Date\", to_date(\"Date\", \"yyyy-MM-dd\"))\n",
        "sales = sales.withColumn(\"Date\", to_date(\"Date\", \"yyyy-MM-dd\"))\n"
      ],
      "metadata": {
        "id": "oF1zsGimPoSS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = sales.join(features, [\"Store\", \"Date\"]).join(stores, [\"Store\"])\n"
      ],
      "metadata": {
        "id": "8xKYkfwyPr54"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n"
      ],
      "metadata": {
        "id": "2Tv3b3MzPuzn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_data = assembler.transform(df).select(\"features\", col(\"Weekly_Sales\").alias(\"label\"))\n"
      ],
      "metadata": {
        "id": "_mLkAH58Pzjc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RetailSalesForecasting\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "Eupv5YTMP2q6"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "# Initialize the model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Train the model\n",
        "lr_model = lr.fit(final_data)\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Coefficients: \", lr_model.coefficients)\n",
        "print(\"Intercept: \", lr_model.intercept)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZge_TPjQPjH",
        "outputId": "8fe1873d-8da0-419b-8526-9b121585b745"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients:  [152.10526316426416,24083.333336564763,-3964.334707182691,0.0]\n",
            "Intercept:  2412169.8780554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = lr_model.transform(final_data)\n",
        "\n",
        "# Evaluate model\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
        "\n",
        "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
        "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
        "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"R²: {r2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQtaadGNQj2_",
        "outputId": "f2e9a8e3-7603-4bcd-9e0c-d132d095984a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 2.0454175357892628e-07\n",
            "MAE: 2.0454172044992447e-07\n",
            "R²: 1.0\n"
          ]
        }
      ]
    }
  ]
}